{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cee5017f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import BertPreTrainedModel, BertConfig, BertModel, BertTokenizer, AdamW\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, RandomSampler, Dataset, SequentialSampler\n",
    "\n",
    "from load_data_ensemble import initialize_data\n",
    "from reading_datasets import read_task5\n",
    "from labels_to_ids import task5_labels_to_ids\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a09c817",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEnsemble(BertPreTrainedModel):\n",
    "  def __init__(self, config, *args, **kwargs):\n",
    "      super().__init__(config)\n",
    "\n",
    "      # model 1\n",
    "      self.bert_model_1 = AutoModelForSequenceClassification.from_pretrained('saved_models/Archive/bert-base-multilingual-cased0')\n",
    "      # model 2\n",
    "      self.bert_model_2 = AutoModelForSequenceClassification.from_pretrained('saved_models/Archive/bert-base-multilingual-cased1')\n",
    "      # combine the 2 models into 1\n",
    "      self.cls = nn.Linear(self.config.hidden_size, 3)\n",
    "      self.init_weights()\n",
    "\n",
    "  def forward(\n",
    "          self,\n",
    "          input_ids=None,\n",
    "          attention_mask=None,\n",
    "          token_type_ids=None,\n",
    "          position_ids=None,\n",
    "          head_mask=None,\n",
    "          inputs_embeds=None,\n",
    "          labels=None,\n",
    "  ):\n",
    "    outputs = []\n",
    "    \n",
    "    input_ids_1 = input_ids[0].to(device, dtype = torch.long)\n",
    "    attention_mask_1 = attention_mask[0].to(device, dtype = torch.long)\n",
    "    labels_1 = labels[0].to(device, dtype = torch.long)\n",
    "    outputs.append(self.bert_model_1(input_ids_1,\n",
    "                                     attention_mask=attention_mask_1, labels =labels_1))\n",
    "\n",
    "    input_ids_2 = input_ids[1].to(device, dtype = torch.long)\n",
    "    attention_mask_2 = attention_mask[1].to(device, dtype = torch.long)\n",
    "    labels_2 = labels[1].to(device, dtype = torch.long)\n",
    "    outputs.append(self.bert_model_2(input_ids_2,\n",
    "                                     attention_mask=attention_mask_2, labels =labels_2))\n",
    "    print(outputs)\n",
    "\n",
    "    # just get the [CLS] embeddings\n",
    "    last_hidden_states = torch.cat([output[1] for output in outputs], dim=1)\n",
    "    print(\"Printing last hidden states\")\n",
    "    print(last_hidden_states)\n",
    "    logits = self.cls(last_hidden_states)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6359a8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "config = BertConfig()\n",
    "model = BertEnsemble(config)\n",
    "model.to(device)\n",
    "learning_rate = 1e-05\n",
    "\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [{\n",
    "  \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "  }]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f822e8",
   "metadata": {},
   "source": [
    "# Training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c6a259d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "[SequenceClassifierOutput(loss=tensor(1.1354, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0180, -0.1941,  0.0535],\n",
      "        [ 0.3210, -0.3704, -0.1389],\n",
      "        [-0.0154, -0.3609, -0.1357],\n",
      "        [-0.1497, -0.0103, -0.1480],\n",
      "        [ 0.0241, -0.2400,  0.0163],\n",
      "        [-0.1895, -0.2245, -0.0981]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=tensor(1.1745, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.4437,  0.0876,  0.1342],\n",
      "        [-0.2217, -0.1233,  0.1770],\n",
      "        [-0.3411,  0.0071,  0.1863],\n",
      "        [-0.5134,  0.2617,  0.0532],\n",
      "        [-0.2329,  0.0977, -0.0128],\n",
      "        [-0.2595,  0.1988, -0.1119]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)]\n",
      "Printing last hidden states\n",
      "tensor([[-0.0180, -0.1941,  0.0535, -0.4437,  0.0876,  0.1342],\n",
      "        [ 0.3210, -0.3704, -0.1389, -0.2217, -0.1233,  0.1770],\n",
      "        [-0.0154, -0.3609, -0.1357, -0.3411,  0.0071,  0.1863],\n",
      "        [-0.1497, -0.0103, -0.1480, -0.5134,  0.2617,  0.0532],\n",
      "        [ 0.0241, -0.2400,  0.0163, -0.2329,  0.0977, -0.0128],\n",
      "        [-0.1895, -0.2245, -0.0981, -0.2595,  0.1988, -0.1119]],\n",
      "       device='cuda:0', grad_fn=<CatBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (6x6 and 768x3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-a166022c5cea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;31m#     # model outputs are always tuple in transformers (see doc)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m#     print(loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pyenv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-7727dde72fdb>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Printing last hidden states\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_hidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_hidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pyenv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pyenv/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pyenv/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (6x6 and 768x3)"
     ]
    }
   ],
   "source": [
    "# standard pytorch way of doing things\n",
    "# 1. create a custom Dataset \n",
    "# 2. pass the dataset to a dataloader\n",
    "# 3. iterate the dataloader and pass the inputs to the model\n",
    "\n",
    "max_len = 256\n",
    "batch_size = 6\n",
    "grad_step = 1\n",
    "initialization_input = (max_len, batch_size)\n",
    "\n",
    "model1_tokenizer = AutoTokenizer.from_pretrained('saved_models/Archive/bert-base-multilingual-cased0')\n",
    "model2_tokenizer = AutoTokenizer.from_pretrained('saved_models/Archive/bert-base-multilingual-cased1')\n",
    "\n",
    "#Reading datasets and initializing data loaders\n",
    "dataset_location = '../2022.07.07_task5/'\n",
    "\n",
    "#Gives us tweet_id, sentence, and label for each dataset.\n",
    "train_data = read_task5(dataset_location , split = 'train')\n",
    "#test_data = read_task5(dataset_location , split = 'dev')#load test set\n",
    "labels_to_ids = task5_labels_to_ids\n",
    "#input_data = (train_data, dev_data, labels_to_ids)\n",
    "\n",
    "dataloader_m1 = initialize_data(model1_tokenizer, initialization_input, train_data, labels_to_ids, shuffle = True)\n",
    "dataloader_m2 = initialize_data(model2_tokenizer, initialization_input, train_data, labels_to_ids, shuffle = True)\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "  # iterate the QA and the AQ inputs simultaneously\n",
    "  for step, combined_batch in enumerate(zip(dataloader_m1, dataloader_m2)):\n",
    "    batch_1, batch_2 = combined_batch\n",
    "    # training so, dropout needed to avoid overfitting\n",
    "    model.train()\n",
    "    # move input to GPU\n",
    "    inputs = {\n",
    "        \"input_ids\": [batch_1['input_ids'], batch_2['input_ids']],\n",
    "        \"attention_mask\": [batch_1['attention_mask'], batch_2['attention_mask']],\n",
    "        \"labels\": [batch_1['labels'], batch_2['labels']]}\n",
    "    print(type(inputs))\n",
    "    \n",
    "    output = model(**inputs)\n",
    "#     # model outputs are always tuple in transformers (see doc)\n",
    "#     print(loss)\n",
    "#     exit()\n",
    "    \n",
    "#     # backpass\n",
    "#     loss.backward()\n",
    "#     print(f\"epoch:{epoch}, loss:{loss}\")\n",
    "    \n",
    "#     # re-calculate the weights\n",
    "#     optimizer.step()\n",
    "#     # again set the grads to 0 for next epoch\n",
    "#     model.zero_grad()\n",
    "  \n",
    "  print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c9f608",
   "metadata": {},
   "source": [
    "# Test Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfbbc545",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prepare_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-8eacf03abee4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# 3. iterate the dataloader and pass the inputs to the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0minput_ids_m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_masks_m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_m1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mtrain_dataset_m1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids_m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_masks_m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'prepare_data' is not defined"
     ]
    }
   ],
   "source": [
    "# standard pytorch way of doing things\n",
    "# 1. create a custom Dataset \n",
    "# 2. pass the dataset to a dataloader\n",
    "# 3. iterate the dataloader and pass the inputs to the model\n",
    "\n",
    "input_ids_m1, attention_masks_m1, labels_m1 = prepare_data(dataset)\n",
    "train_dataset_m1 = dataset(input_ids_m1, attention_masks_m1, labels_m1)\n",
    "\n",
    "input_ids_m2, attention_masks_m2, labels_m2 = prepare_data(dataset)\n",
    "train_dataset_m2 = dataset(input_ids_m2, attention_masks_m2, labels_m2)\n",
    "\n",
    "dataloader_m1 =  DataLoader(dataset=train_dataset_m1, \n",
    "                            batch_size=10, \n",
    "                            sampler=SequentialSampler(train_dataset_m1))\n",
    "dataloader_m2 =  DataLoader(dataset=train_dataset_m2, \n",
    "                            batch_size=10, \n",
    "                            sampler=SequentialSampler(train_dataset_m2))\n",
    "\n",
    "complete_outputs, complete_label_ids = [], []\n",
    "\n",
    "# iterate the QA and the AQ inputs simultaneously\n",
    "for step, combined_batch in enumerate(zip(dataloader_m1, dataloader_m2)):\n",
    "  # only forward pass so no dropout\n",
    "  model.eval()\n",
    "  batch_1, batch_2 = combined_batch\n",
    "\n",
    "  # move input to GPU\n",
    "  batch_1 = tuple(t.to(device) for t in batch_1)\n",
    "  batch_2 = tuple(t.to(device) for t in batch_2)\n",
    "\n",
    "  # no back pass so no need to track variables for differentiation\n",
    "  with torch.no_grad():\n",
    "    inputs = {\n",
    "        \"input_ids\": [batch_1[0], batch_2[0]],\n",
    "        \"attention_mask\": [batch_1[1], batch_2[1]],\n",
    "        \"labels\": [batch_1[2], batch_2[2]]\n",
    "    }\n",
    "    outputs = model(**inputs)\n",
    "    tmp_eval_loss, logits = outputs[:2]\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    outputs = np.argmax(logits, axis=1)\n",
    "    label_ids = inputs[\"labels\"].detach().cpu().numpy()\n",
    "  complete_outputs.extend(outputs)\n",
    "  complete_label_ids.extend(label_ids)\n",
    "\n",
    "print(complete_outputs, complete_label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c9d522",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
