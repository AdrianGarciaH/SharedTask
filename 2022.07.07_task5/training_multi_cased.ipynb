{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7ea7f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import cuda\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score\n",
    "from load_data import initialize_data\n",
    "from reading_datasets import read_task5\n",
    "from labels_to_ids import task5_labels_to_ids\n",
    "import time\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcf25741",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, train_loader, model, optimizer, device, grad_step = 1, max_grad_norm = 10):\n",
    "    tr_loss, tr_accuracy = 0, 0\n",
    "    tr_precision, tr_recall = 0, 0\n",
    "    tr_f1score = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    tr_preds, tr_labels = [], []\n",
    "    # put model in training mode\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for idx, batch in enumerate(train_loader):\n",
    "        ids = batch['input_ids'].to(device, dtype = torch.long)\n",
    "        mask = batch['attention_mask'].to(device, dtype = torch.long)\n",
    "        labels = batch['labels'].to(device, dtype = torch.long)\n",
    "\n",
    "        if (idx + 1) % 20 == 0:\n",
    "            print('FINSIHED BATCH:', idx, 'of', len(train_loader))\n",
    "\n",
    "        #loss, tr_logits = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
    "        output = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
    "        tr_loss += output[0]\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples += labels.size(0)\n",
    "           \n",
    "        # compute training accuracy\n",
    "        flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n",
    "        active_logits = output[1].view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
    "        \n",
    "        # only compute accuracy at active labels\n",
    "        active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n",
    "        #active_labels = torch.where(active_accuracy, labels.view(-1), torch.tensor(-100).type_as(labels))\n",
    "        \n",
    "        labels = torch.masked_select(flattened_targets, active_accuracy)\n",
    "        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "\n",
    "        tr_labels.extend(labels)\n",
    "        tr_preds.extend(predictions)\n",
    "\n",
    "        tmp_tr_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n",
    "        tr_accuracy += tmp_tr_accuracy\n",
    "        \n",
    "        # Compute Precision\n",
    "        tmp_tr_precision = precision_score(labels.cpu().numpy(), predictions.cpu().numpy(), labels=[0,1,2], average = None, zero_division=0 )[2]\n",
    "        tr_precision += tmp_tr_precision\n",
    "        \n",
    "        # Compute Recall\n",
    "        tmp_tr_recall = recall_score(labels.cpu().numpy(), predictions.cpu().numpy(), labels=[0,1,2], average = None, zero_division=0 )[2]\n",
    "        tr_recall += tmp_tr_recall\n",
    "        \n",
    "        # Compute f1score\n",
    "        tmp_tr_f1score = f1_score(labels.cpu().numpy(), predictions.cpu().numpy(),labels=[0,1,2], average=None, zero_division=0)[2]\n",
    "        tr_f1score += tmp_tr_f1score\n",
    "    \n",
    "\n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            parameters=model.parameters(), max_norm=max_grad_norm\n",
    "        )\n",
    "        \n",
    "        # backward pass\n",
    "        output['loss'].backward()\n",
    "        if (idx + 1) % grad_step == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    epoch_loss = tr_loss / nb_tr_steps\n",
    "    tr_accuracy = tr_accuracy / nb_tr_steps\n",
    "    tr_precision = tr_precision / nb_tr_steps\n",
    "    tr_recall = tr_recall / nb_tr_steps\n",
    "    tr_f1score= tr_f1score / nb_tr_steps\n",
    "    #print(f\"Training loss epoch: {epoch_loss}\")\n",
    "    #print(f\"Training accuracy epoch: {tr_accuracy}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "562ddd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(model, testing_loader, labels_to_ids, device):\n",
    "    # put model in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    eval_precision, eval_recall = 0, 0\n",
    "    eval_f1score = 0\n",
    "    nb_eval_examples, nb_eval_steps = 0, 0\n",
    "    eval_preds, eval_labels = [], []\n",
    "     \n",
    "    \n",
    "    ids_to_labels = dict((v,k) for k,v in labels_to_ids.items())\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(testing_loader):\n",
    "            \n",
    "            ids = batch['input_ids'].to(device, dtype = torch.long)\n",
    "            mask = batch['attention_mask'].to(device, dtype = torch.long)\n",
    "            labels = batch['labels'].to(device, dtype = torch.long)\n",
    "            \n",
    "            #loss, eval_logits = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
    "            output = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
    "\n",
    "            eval_loss += output['loss'].item()\n",
    "\n",
    "            nb_eval_steps += 1\n",
    "            nb_eval_examples += labels.size(0)\n",
    "        \n",
    "            if idx % 100==0:\n",
    "                loss_step = eval_loss/nb_eval_steps\n",
    "                print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n",
    "              \n",
    "            # compute evaluation accuracy\n",
    "            flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n",
    "            active_logits = output[1].view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
    "            \n",
    "            # only compute accuracy at active labels\n",
    "            active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n",
    "        \n",
    "            labels = torch.masked_select(flattened_targets, active_accuracy)\n",
    "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "            \n",
    "            eval_labels.extend(labels)\n",
    "            eval_preds.extend(predictions)\n",
    "            \n",
    "            tmp_eval_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "            \n",
    "            # Compute Precision\n",
    "            tmp_eval_precision = precision_score(labels.cpu().numpy(), predictions.cpu().numpy(), labels=[0,1,2], average = None, zero_division=0)[2]\n",
    "            eval_precision += tmp_eval_precision\n",
    "            \n",
    "            # Compute Recall\n",
    "            tmp_eval_recall = recall_score(labels.cpu().numpy(), predictions.cpu().numpy(), labels=[0,1,2], average = None, zero_division=0)[2]\n",
    "            eval_recall += tmp_eval_recall\n",
    "            \n",
    "            # Compute f1score\n",
    "            tmp_eval_f1score = f1_score(labels.cpu().numpy(), predictions.cpu().numpy(),labels=[0,1,2], average=None, zero_division=0)[2]\n",
    "            eval_f1score += tmp_eval_f1score\n",
    "\n",
    "    labels = [ids_to_labels[id.item()] for id in eval_labels]\n",
    "    predictions = [ids_to_labels[id.item()] for id in eval_preds]\n",
    "    \n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    eval_accuracy = eval_accuracy / nb_eval_steps\n",
    "    eval_precision = eval_precision / nb_eval_steps\n",
    "    eval_recall = eval_recall / nb_eval_steps\n",
    "    eval_f1score = eval_f1score / nb_eval_steps\n",
    "    #print(f\"Validation Loss: {eval_loss}\")\n",
    "    #print(f\"Validation Accuracy: {eval_accuracy}\")\n",
    "\n",
    "    return labels, predictions, eval_accuracy, eval_precision, eval_recall, eval_f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74d3ba5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(n_epochs, model_name, model_save_flag, model_save_location, model_load_flag, model_load_location):\n",
    "    #Initialization training parameters\n",
    "    max_len = 256\n",
    "    batch_size = 32\n",
    "    grad_step = 1\n",
    "    learning_rate = 1e-05\n",
    "    initialization_input = (max_len, batch_size)\n",
    "\n",
    "    #Reading datasets and initializing data loaders\n",
    "    dataset_location = '../2022.07.07_task5/'\n",
    "\n",
    "    train_data = read_task5(dataset_location , split = 'train')\n",
    "    dev_data = read_task5(dataset_location , split = 'dev')\n",
    "    #test_data = read_task5(dataset_location , split = 'dev')#load test set\n",
    "    labels_to_ids = task5_labels_to_ids\n",
    "    #input_data = (train_data, dev_data, labels_to_ids)\n",
    "\n",
    "    #Define tokenizer, model and optimizer\n",
    "    device = 'cuda' if cuda.is_available() else 'cpu' #save the processing time\n",
    "    if model_load_flag:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_load_location)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_load_location)\n",
    "    else: \n",
    "        tokenizer =  AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(labels_to_ids))\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "    model.to(device)\n",
    "\n",
    "    #Get dataloaders\n",
    "    train_loader = initialize_data(tokenizer, initialization_input, train_data, labels_to_ids, shuffle = True)\n",
    "    dev_loader = initialize_data(tokenizer, initialization_input, dev_data, labels_to_ids, shuffle = True)\n",
    "    #test_loader = initialize_data(tokenizer, initialization_input, test_data, labels_to_ids, shuffle = True)#create test loader\n",
    "\n",
    "    best_dev_acc = 0\n",
    "    best_test_acc = 0\n",
    "    best_dev_precision = 0\n",
    "    best_test_precision = 0\n",
    "    best_dev_recall = 0\n",
    "    best_test_recall = 0\n",
    "    best_dev_f1score = 0\n",
    "    best_test_f1score = 0\n",
    "    best_epoch = -1\n",
    "    \n",
    "    list_dev_acc = [] \n",
    "    list_test_acc = []  \n",
    "    list_dev_precision = []  \n",
    "    list_test_precision  = []  \n",
    "    list_dev_recall = []  \n",
    "    list_test_recall = []  \n",
    "    list_dev_f1score = []  \n",
    "    list_test_f1score = []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        start = time.time()\n",
    "        print(f\"Training epoch: {epoch + 1}\")\n",
    "\n",
    "        #train model\n",
    "        if not model_load_flag:\n",
    "            model = train(epoch, train_loader, model, optimizer, device, grad_step)\n",
    "        \n",
    "        #testing and logging\n",
    "        labels_dev, predictions_dev, dev_accuracy, dev_precision, dev_recall, dev_f1score = testing(model, dev_loader, labels_to_ids, device)\n",
    "        print('DEV ACC:', dev_accuracy)\n",
    "        print('DEV Precision:' , dev_precision)\n",
    "        print('DEV Recall:' , dev_recall)\n",
    "        print('DEV F1Score:' , dev_f1score)\n",
    "        \n",
    "        list_dev_acc.append(dev_accuracy)     \n",
    "        list_dev_precision.append(dev_precision)   \n",
    "        list_dev_recall.append(dev_recall)  \n",
    "        list_dev_f1score.append(dev_f1score)  \n",
    "        \n",
    "        \n",
    "        #labels_test, predictions_test, test_accuracy, test_precision, test_recall, test_f1score = testing(model, test_loader, labels_to_ids, device)\n",
    "        #print('TEST ACC:', test_accuracy)\n",
    "        #print('TEST Precision:' , test_precision)\n",
    "        #print('TEST Recall:' , test_recall)\n",
    "        #print('TEST F1Score:' , test_f1score)\n",
    "        \n",
    "        #list_test_acc.append(test_accuracy) \n",
    "        #list_test_precision.append(test_precision)  \n",
    "        #list_test_recall.append(test_recall)\n",
    "        #list_test_f1score.append(test_f1score) \n",
    "\n",
    "        #saving model\n",
    "        if dev_accuracy > best_dev_acc:\n",
    "            best_dev_acc = dev_accuracy\n",
    "            #best_test_acc = test_accuracy\n",
    "        if dev_precision > best_dev_precision:\n",
    "            best_dev_precision = dev_precision\n",
    "            #best_test_precision = test_precision\n",
    "        if dev_recall > best_dev_recall:\n",
    "            best_dev_recall = dev_recall\n",
    "            #best_test_recall = test_recall\n",
    "        if dev_f1score > best_dev_f1score:\n",
    "            best_dev_f1score = dev_f1score\n",
    "            #best_test_f1score = test_f1score\n",
    "            best_epoch = epoch\n",
    "            \n",
    "            if model_save_flag:\n",
    "                os.makedirs(model_save_location, exist_ok=True)\n",
    "                tokenizer.save_pretrained(model_save_location)\n",
    "                model.save_pretrained(model_save_location)\n",
    "\n",
    "        now = time.time()\n",
    "        print('BEST ACCURACY --> ', 'DEV:', round(best_dev_acc, 5))\n",
    "        print('BEST PRECISION --> ', 'DEV:', round(best_dev_precision, 5))\n",
    "        print('BEST RECALL --> ', 'DEV:', round(best_dev_recall, 5))\n",
    "        print('BEST F1SCORE --> ', 'DEV:', round(best_dev_f1score, 5))\n",
    "        print('TIME PER EPOCH:', (now-start)/60 )\n",
    "        print()\n",
    "\n",
    "    return best_dev_acc, best_test_acc, best_epoch, best_dev_precision, best_test_precision, best_dev_recall, best_test_recall, best_dev_f1score, best_test_f1score, list_dev_acc, list_test_acc, list_dev_precision, list_test_precision, list_dev_recall, list_test_recall, list_dev_f1score, list_test_f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d77597b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1\n",
      "FINSIHED BATCH: 19 of 252\n",
      "FINSIHED BATCH: 39 of 252\n",
      "FINSIHED BATCH: 59 of 252\n",
      "FINSIHED BATCH: 79 of 252\n",
      "FINSIHED BATCH: 99 of 252\n",
      "FINSIHED BATCH: 119 of 252\n",
      "FINSIHED BATCH: 139 of 252\n",
      "FINSIHED BATCH: 159 of 252\n",
      "FINSIHED BATCH: 179 of 252\n",
      "FINSIHED BATCH: 199 of 252\n",
      "FINSIHED BATCH: 219 of 252\n",
      "FINSIHED BATCH: 239 of 252\n",
      "Validation loss per 100 evaluation steps: 0.20034973323345184\n",
      "DEV ACC: 0.8236331569664902\n",
      "DEV Precision: 0.6691403834260978\n",
      "DEV Recall: 0.8005165028974551\n",
      "DEV F1Score: 0.7124427001951694\n",
      "BEST ACCURACY -->  DEV: 0.82363\n",
      "BEST PRECISION -->  DEV: 0.66914\n",
      "BEST RECALL -->  DEV: 0.80052\n",
      "BEST F1SCORE -->  DEV: 0.71244\n",
      "TIME PER EPOCH: 6.709881325562795\n",
      "\n",
      "Training epoch: 2\n",
      "FINSIHED BATCH: 19 of 252\n",
      "FINSIHED BATCH: 39 of 252\n",
      "FINSIHED BATCH: 59 of 252\n",
      "FINSIHED BATCH: 79 of 252\n",
      "FINSIHED BATCH: 99 of 252\n",
      "FINSIHED BATCH: 119 of 252\n",
      "FINSIHED BATCH: 139 of 252\n",
      "FINSIHED BATCH: 159 of 252\n",
      "FINSIHED BATCH: 179 of 252\n",
      "FINSIHED BATCH: 199 of 252\n",
      "FINSIHED BATCH: 219 of 252\n",
      "FINSIHED BATCH: 239 of 252\n",
      "Validation loss per 100 evaluation steps: 0.4522797167301178\n",
      "DEV ACC: 0.8194811875367431\n",
      "DEV Precision: 0.7099521289997481\n",
      "DEV Recall: 0.5489103048626857\n",
      "DEV F1Score: 0.601333322761894\n",
      "BEST ACCURACY -->  DEV: 0.82363\n",
      "BEST PRECISION -->  DEV: 0.70995\n",
      "BEST RECALL -->  DEV: 0.80052\n",
      "BEST F1SCORE -->  DEV: 0.71244\n",
      "TIME PER EPOCH: 6.862987645467123\n",
      "\n",
      "Training epoch: 3\n",
      "FINSIHED BATCH: 19 of 252\n",
      "FINSIHED BATCH: 39 of 252\n",
      "FINSIHED BATCH: 59 of 252\n",
      "FINSIHED BATCH: 79 of 252\n",
      "FINSIHED BATCH: 99 of 252\n",
      "FINSIHED BATCH: 119 of 252\n",
      "FINSIHED BATCH: 139 of 252\n",
      "FINSIHED BATCH: 159 of 252\n",
      "FINSIHED BATCH: 179 of 252\n",
      "FINSIHED BATCH: 199 of 252\n",
      "FINSIHED BATCH: 219 of 252\n",
      "FINSIHED BATCH: 239 of 252\n",
      "Validation loss per 100 evaluation steps: 0.22224920988082886\n",
      "DEV ACC: 0.8398184891240447\n",
      "DEV Precision: 0.7103237591332828\n",
      "DEV Recall: 0.7355568153187202\n",
      "DEV F1Score: 0.7057421906861681\n",
      "BEST ACCURACY -->  DEV: 0.83982\n",
      "BEST PRECISION -->  DEV: 0.71032\n",
      "BEST RECALL -->  DEV: 0.80052\n",
      "BEST F1SCORE -->  DEV: 0.71244\n",
      "TIME PER EPOCH: 6.863847680886587\n",
      "\n",
      "Training epoch: 4\n",
      "FINSIHED BATCH: 19 of 252\n",
      "FINSIHED BATCH: 39 of 252\n",
      "FINSIHED BATCH: 59 of 252\n",
      "FINSIHED BATCH: 79 of 252\n",
      "FINSIHED BATCH: 99 of 252\n",
      "FINSIHED BATCH: 119 of 252\n",
      "FINSIHED BATCH: 139 of 252\n",
      "FINSIHED BATCH: 159 of 252\n",
      "FINSIHED BATCH: 179 of 252\n",
      "FINSIHED BATCH: 199 of 252\n",
      "FINSIHED BATCH: 219 of 252\n",
      "FINSIHED BATCH: 239 of 252\n",
      "Validation loss per 100 evaluation steps: 0.7776805758476257\n",
      "DEV ACC: 0.8401859200470312\n",
      "DEV Precision: 0.6895920657825421\n",
      "DEV Recall: 0.7699105568153187\n",
      "DEV F1Score: 0.7127568740387549\n",
      "BEST ACCURACY -->  DEV: 0.84019\n",
      "BEST PRECISION -->  DEV: 0.71032\n",
      "BEST RECALL -->  DEV: 0.80052\n",
      "BEST F1SCORE -->  DEV: 0.71276\n",
      "TIME PER EPOCH: 6.890837208429972\n",
      "\n",
      "Training epoch: 5\n",
      "FINSIHED BATCH: 19 of 252\n",
      "FINSIHED BATCH: 39 of 252\n",
      "FINSIHED BATCH: 59 of 252\n",
      "FINSIHED BATCH: 79 of 252\n",
      "FINSIHED BATCH: 99 of 252\n",
      "FINSIHED BATCH: 119 of 252\n",
      "FINSIHED BATCH: 139 of 252\n",
      "FINSIHED BATCH: 159 of 252\n",
      "FINSIHED BATCH: 179 of 252\n",
      "FINSIHED BATCH: 199 of 252\n",
      "FINSIHED BATCH: 219 of 252\n",
      "FINSIHED BATCH: 239 of 252\n",
      "Validation loss per 100 evaluation steps: 0.4292362630367279\n",
      "DEV ACC: 0.8401308054085832\n",
      "DEV Precision: 0.6898686410591176\n",
      "DEV Recall: 0.7784706475182666\n",
      "DEV F1Score: 0.708495612697293\n",
      "BEST ACCURACY -->  DEV: 0.84019\n",
      "BEST PRECISION -->  DEV: 0.71032\n",
      "BEST RECALL -->  DEV: 0.80052\n",
      "BEST F1SCORE -->  DEV: 0.71276\n",
      "TIME PER EPOCH: 6.865419121583303\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1\n",
      "FINSIHED BATCH: 19 of 252\n",
      "FINSIHED BATCH: 39 of 252\n",
      "FINSIHED BATCH: 59 of 252\n",
      "FINSIHED BATCH: 79 of 252\n",
      "FINSIHED BATCH: 99 of 252\n",
      "FINSIHED BATCH: 119 of 252\n",
      "FINSIHED BATCH: 139 of 252\n",
      "FINSIHED BATCH: 159 of 252\n",
      "FINSIHED BATCH: 179 of 252\n",
      "FINSIHED BATCH: 199 of 252\n",
      "FINSIHED BATCH: 219 of 252\n",
      "FINSIHED BATCH: 239 of 252\n",
      "Validation loss per 100 evaluation steps: 0.3047519624233246\n",
      "DEV ACC: 0.8147045855379188\n",
      "DEV Precision: 0.6191982180077417\n",
      "DEV Recall: 0.8946694839551981\n",
      "DEV F1Score: 0.7148121086791288\n",
      "BEST ACCURACY -->  DEV: 0.8147\n",
      "BEST PRECISION -->  DEV: 0.6192\n",
      "BEST RECALL -->  DEV: 0.89467\n",
      "BEST F1SCORE -->  DEV: 0.71481\n",
      "TIME PER EPOCH: 6.844246486822764\n",
      "\n",
      "Training epoch: 2\n",
      "FINSIHED BATCH: 19 of 252\n",
      "FINSIHED BATCH: 39 of 252\n",
      "FINSIHED BATCH: 59 of 252\n",
      "FINSIHED BATCH: 79 of 252\n",
      "FINSIHED BATCH: 99 of 252\n",
      "FINSIHED BATCH: 119 of 252\n",
      "FINSIHED BATCH: 139 of 252\n",
      "FINSIHED BATCH: 159 of 252\n",
      "FINSIHED BATCH: 179 of 252\n",
      "FINSIHED BATCH: 199 of 252\n",
      "FINSIHED BATCH: 219 of 252\n",
      "FINSIHED BATCH: 239 of 252\n",
      "Validation loss per 100 evaluation steps: 0.34166792035102844\n",
      "DEV ACC: 0.8278218694885362\n",
      "DEV Precision: 0.7446082136558327\n",
      "DEV Recall: 0.6223293020912071\n",
      "DEV F1Score: 0.6592020413448986\n",
      "BEST ACCURACY -->  DEV: 0.82782\n",
      "BEST PRECISION -->  DEV: 0.74461\n",
      "BEST RECALL -->  DEV: 0.89467\n",
      "BEST F1SCORE -->  DEV: 0.71481\n",
      "TIME PER EPOCH: 6.795544020334879\n",
      "\n",
      "Training epoch: 3\n",
      "FINSIHED BATCH: 19 of 252\n",
      "FINSIHED BATCH: 39 of 252\n",
      "FINSIHED BATCH: 59 of 252\n",
      "FINSIHED BATCH: 79 of 252\n",
      "FINSIHED BATCH: 99 of 252\n",
      "FINSIHED BATCH: 119 of 252\n",
      "FINSIHED BATCH: 139 of 252\n",
      "FINSIHED BATCH: 159 of 252\n",
      "FINSIHED BATCH: 179 of 252\n",
      "FINSIHED BATCH: 199 of 252\n",
      "FINSIHED BATCH: 219 of 252\n",
      "FINSIHED BATCH: 239 of 252\n",
      "Validation loss per 100 evaluation steps: 0.2804441452026367\n",
      "DEV ACC: 0.8458627278071723\n",
      "DEV Precision: 0.6786911060720585\n",
      "DEV Recall: 0.8079805996472663\n",
      "DEV F1Score: 0.7232984944137825\n",
      "BEST ACCURACY -->  DEV: 0.84586\n",
      "BEST PRECISION -->  DEV: 0.74461\n",
      "BEST RECALL -->  DEV: 0.89467\n",
      "BEST F1SCORE -->  DEV: 0.7233\n",
      "TIME PER EPOCH: 6.826768906911214\n",
      "\n",
      "Training epoch: 4\n",
      "FINSIHED BATCH: 19 of 252\n",
      "FINSIHED BATCH: 39 of 252\n",
      "FINSIHED BATCH: 59 of 252\n",
      "FINSIHED BATCH: 79 of 252\n",
      "FINSIHED BATCH: 99 of 252\n",
      "FINSIHED BATCH: 119 of 252\n",
      "FINSIHED BATCH: 139 of 252\n",
      "FINSIHED BATCH: 159 of 252\n",
      "FINSIHED BATCH: 179 of 252\n",
      "FINSIHED BATCH: 199 of 252\n",
      "FINSIHED BATCH: 219 of 252\n",
      "FINSIHED BATCH: 239 of 252\n",
      "Validation loss per 100 evaluation steps: 0.24761827290058136\n",
      "DEV ACC: 0.8312022339800118\n",
      "DEV Precision: 0.663394718156623\n",
      "DEV Recall: 0.8277955289860051\n",
      "DEV F1Score: 0.7188054398138431\n",
      "BEST ACCURACY -->  DEV: 0.84586\n",
      "BEST PRECISION -->  DEV: 0.74461\n",
      "BEST RECALL -->  DEV: 0.89467\n",
      "BEST F1SCORE -->  DEV: 0.7233\n",
      "TIME PER EPOCH: 6.782942561308543\n",
      "\n",
      "Training epoch: 5\n",
      "FINSIHED BATCH: 19 of 252\n",
      "FINSIHED BATCH: 39 of 252\n",
      "FINSIHED BATCH: 59 of 252\n",
      "FINSIHED BATCH: 79 of 252\n",
      "FINSIHED BATCH: 99 of 252\n",
      "FINSIHED BATCH: 119 of 252\n",
      "FINSIHED BATCH: 139 of 252\n",
      "FINSIHED BATCH: 159 of 252\n",
      "FINSIHED BATCH: 179 of 252\n",
      "FINSIHED BATCH: 199 of 252\n",
      "FINSIHED BATCH: 219 of 252\n",
      "FINSIHED BATCH: 239 of 252\n",
      "Validation loss per 100 evaluation steps: 0.43605944514274597\n",
      "DEV ACC: 0.841086125808348\n",
      "DEV Precision: 0.6492379313807886\n",
      "DEV Recall: 0.9022234819853866\n",
      "DEV F1Score: 0.7438834187691623\n",
      "BEST ACCURACY -->  DEV: 0.84586\n",
      "BEST PRECISION -->  DEV: 0.74461\n",
      "BEST RECALL -->  DEV: 0.90222\n",
      "BEST F1SCORE -->  DEV: 0.74388\n",
      "TIME PER EPOCH: 6.800163801511129\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1\n",
      "FINSIHED BATCH: 19 of 252\n",
      "FINSIHED BATCH: 39 of 252\n",
      "FINSIHED BATCH: 59 of 252\n",
      "FINSIHED BATCH: 79 of 252\n",
      "FINSIHED BATCH: 99 of 252\n",
      "FINSIHED BATCH: 119 of 252\n",
      "FINSIHED BATCH: 139 of 252\n",
      "FINSIHED BATCH: 159 of 252\n",
      "FINSIHED BATCH: 179 of 252\n",
      "FINSIHED BATCH: 199 of 252\n",
      "FINSIHED BATCH: 219 of 252\n",
      "FINSIHED BATCH: 239 of 252\n",
      "Validation loss per 100 evaluation steps: 0.4000139832496643\n",
      "DEV ACC: 0.8347295708406819\n",
      "DEV Precision: 0.7025991204562633\n",
      "DEV Recall: 0.8053539934492315\n",
      "DEV F1Score: 0.7340268404694171\n",
      "BEST ACCURACY -->  DEV: 0.83473\n",
      "BEST PRECISION -->  DEV: 0.7026\n",
      "BEST RECALL -->  DEV: 0.80535\n",
      "BEST F1SCORE -->  DEV: 0.73403\n",
      "TIME PER EPOCH: 6.788865256309509\n",
      "\n",
      "Training epoch: 2\n",
      "FINSIHED BATCH: 19 of 252\n",
      "FINSIHED BATCH: 39 of 252\n",
      "FINSIHED BATCH: 59 of 252\n",
      "FINSIHED BATCH: 79 of 252\n",
      "FINSIHED BATCH: 99 of 252\n",
      "FINSIHED BATCH: 119 of 252\n",
      "FINSIHED BATCH: 139 of 252\n",
      "FINSIHED BATCH: 159 of 252\n",
      "FINSIHED BATCH: 179 of 252\n",
      "FINSIHED BATCH: 199 of 252\n",
      "FINSIHED BATCH: 219 of 252\n",
      "FINSIHED BATCH: 239 of 252\n",
      "Validation loss per 100 evaluation steps: 0.32847505807876587\n",
      "DEV ACC: 0.8225492357436802\n",
      "DEV Precision: 0.6329598531979487\n",
      "DEV Recall: 0.9126417233560091\n",
      "DEV F1Score: 0.7319314151379928\n",
      "BEST ACCURACY -->  DEV: 0.83473\n",
      "BEST PRECISION -->  DEV: 0.7026\n",
      "BEST RECALL -->  DEV: 0.91264\n",
      "BEST F1SCORE -->  DEV: 0.73403\n",
      "TIME PER EPOCH: 6.8012003620465595\n",
      "\n",
      "Training epoch: 3\n",
      "FINSIHED BATCH: 19 of 252\n",
      "FINSIHED BATCH: 39 of 252\n",
      "FINSIHED BATCH: 59 of 252\n",
      "FINSIHED BATCH: 79 of 252\n",
      "FINSIHED BATCH: 99 of 252\n",
      "FINSIHED BATCH: 119 of 252\n",
      "FINSIHED BATCH: 139 of 252\n",
      "FINSIHED BATCH: 159 of 252\n",
      "FINSIHED BATCH: 179 of 252\n",
      "FINSIHED BATCH: 199 of 252\n",
      "FINSIHED BATCH: 219 of 252\n",
      "FINSIHED BATCH: 239 of 252\n",
      "Validation loss per 100 evaluation steps: 0.36308589577674866\n",
      "DEV ACC: 0.8419863315696648\n",
      "DEV Precision: 0.7139438603724317\n",
      "DEV Recall: 0.6653088534040915\n",
      "DEV F1Score: 0.6690647597370926\n",
      "BEST ACCURACY -->  DEV: 0.84199\n",
      "BEST PRECISION -->  DEV: 0.71394\n",
      "BEST RECALL -->  DEV: 0.91264\n",
      "BEST F1SCORE -->  DEV: 0.73403\n",
      "TIME PER EPOCH: 6.804373252391815\n",
      "\n",
      "Training epoch: 4\n",
      "FINSIHED BATCH: 19 of 252\n",
      "FINSIHED BATCH: 39 of 252\n",
      "FINSIHED BATCH: 59 of 252\n",
      "FINSIHED BATCH: 79 of 252\n",
      "FINSIHED BATCH: 99 of 252\n",
      "FINSIHED BATCH: 119 of 252\n",
      "FINSIHED BATCH: 139 of 252\n",
      "FINSIHED BATCH: 159 of 252\n",
      "FINSIHED BATCH: 179 of 252\n",
      "FINSIHED BATCH: 199 of 252\n",
      "FINSIHED BATCH: 219 of 252\n",
      "FINSIHED BATCH: 239 of 252\n",
      "Validation loss per 100 evaluation steps: 0.21023845672607422\n",
      "DEV ACC: 0.8422986478542034\n",
      "DEV Precision: 0.6624040862136099\n",
      "DEV Recall: 0.8693625598387502\n",
      "DEV F1Score: 0.7402936333682314\n",
      "BEST ACCURACY -->  DEV: 0.8423\n",
      "BEST PRECISION -->  DEV: 0.71394\n",
      "BEST RECALL -->  DEV: 0.91264\n",
      "BEST F1SCORE -->  DEV: 0.74029\n",
      "TIME PER EPOCH: 6.8366246819496155\n",
      "\n",
      "Training epoch: 5\n",
      "FINSIHED BATCH: 19 of 252\n",
      "FINSIHED BATCH: 39 of 252\n",
      "FINSIHED BATCH: 59 of 252\n",
      "FINSIHED BATCH: 79 of 252\n",
      "FINSIHED BATCH: 99 of 252\n",
      "FINSIHED BATCH: 119 of 252\n",
      "FINSIHED BATCH: 139 of 252\n",
      "FINSIHED BATCH: 159 of 252\n",
      "FINSIHED BATCH: 179 of 252\n",
      "FINSIHED BATCH: 199 of 252\n",
      "FINSIHED BATCH: 219 of 252\n",
      "FINSIHED BATCH: 239 of 252\n",
      "Validation loss per 100 evaluation steps: 0.4589528441429138\n",
      "DEV ACC: 0.8356665196942975\n",
      "DEV Precision: 0.6605321926750498\n",
      "DEV Recall: 0.7997480473670948\n",
      "DEV F1Score: 0.7052597690884583\n",
      "BEST ACCURACY -->  DEV: 0.8423\n",
      "BEST PRECISION -->  DEV: 0.71394\n",
      "BEST RECALL -->  DEV: 0.91264\n",
      "BEST F1SCORE -->  DEV: 0.74029\n",
      "TIME PER EPOCH: 6.752744813760121\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1\n",
      "FINSIHED BATCH: 19 of 252\n",
      "FINSIHED BATCH: 39 of 252\n",
      "FINSIHED BATCH: 59 of 252\n",
      "FINSIHED BATCH: 79 of 252\n",
      "FINSIHED BATCH: 99 of 252\n",
      "FINSIHED BATCH: 119 of 252\n",
      "FINSIHED BATCH: 139 of 252\n",
      "FINSIHED BATCH: 159 of 252\n",
      "FINSIHED BATCH: 179 of 252\n",
      "FINSIHED BATCH: 199 of 252\n",
      "FINSIHED BATCH: 219 of 252\n",
      "FINSIHED BATCH: 239 of 252\n",
      "Validation loss per 100 evaluation steps: 0.3349117636680603\n",
      "DEV ACC: 0.8180849500293945\n",
      "DEV Precision: 0.6484018186399139\n",
      "DEV Recall: 0.8416099773242628\n",
      "DEV F1Score: 0.7200731236482377\n",
      "BEST ACCURACY -->  DEV: 0.81808\n",
      "BEST PRECISION -->  DEV: 0.6484\n",
      "BEST RECALL -->  DEV: 0.84161\n",
      "BEST F1SCORE -->  DEV: 0.72007\n",
      "TIME PER EPOCH: 6.844860637187958\n",
      "\n",
      "Training epoch: 2\n",
      "FINSIHED BATCH: 19 of 252\n",
      "FINSIHED BATCH: 39 of 252\n",
      "FINSIHED BATCH: 59 of 252\n",
      "FINSIHED BATCH: 79 of 252\n",
      "FINSIHED BATCH: 99 of 252\n",
      "FINSIHED BATCH: 119 of 252\n",
      "FINSIHED BATCH: 139 of 252\n",
      "FINSIHED BATCH: 159 of 252\n",
      "FINSIHED BATCH: 179 of 252\n",
      "FINSIHED BATCH: 199 of 252\n",
      "FINSIHED BATCH: 219 of 252\n",
      "FINSIHED BATCH: 239 of 252\n",
      "Validation loss per 100 evaluation steps: 0.28585177659988403\n",
      "DEV ACC: 0.8365299823633157\n",
      "DEV Precision: 0.6690361666552144\n",
      "DEV Recall: 0.84937641723356\n",
      "DEV F1Score: 0.7340460216584346\n",
      "BEST ACCURACY -->  DEV: 0.83653\n",
      "BEST PRECISION -->  DEV: 0.66904\n",
      "BEST RECALL -->  DEV: 0.84938\n",
      "BEST F1SCORE -->  DEV: 0.73405\n",
      "TIME PER EPOCH: 6.879648919900259\n",
      "\n",
      "Training epoch: 3\n",
      "FINSIHED BATCH: 19 of 252\n",
      "FINSIHED BATCH: 39 of 252\n",
      "FINSIHED BATCH: 59 of 252\n",
      "FINSIHED BATCH: 79 of 252\n",
      "FINSIHED BATCH: 99 of 252\n",
      "FINSIHED BATCH: 119 of 252\n",
      "FINSIHED BATCH: 139 of 252\n",
      "FINSIHED BATCH: 159 of 252\n",
      "FINSIHED BATCH: 179 of 252\n",
      "FINSIHED BATCH: 199 of 252\n",
      "FINSIHED BATCH: 219 of 252\n",
      "FINSIHED BATCH: 239 of 252\n",
      "Validation loss per 100 evaluation steps: 0.4315285384654999\n",
      "DEV ACC: 0.8430702527924749\n",
      "DEV Precision: 0.6888001328477519\n",
      "DEV Recall: 0.8306248425296043\n",
      "DEV F1Score: 0.7371211682099192\n",
      "BEST ACCURACY -->  DEV: 0.84307\n",
      "BEST PRECISION -->  DEV: 0.6888\n",
      "BEST RECALL -->  DEV: 0.84938\n",
      "BEST F1SCORE -->  DEV: 0.73712\n",
      "TIME PER EPOCH: 6.885120562712351\n",
      "\n",
      "Training epoch: 4\n",
      "FINSIHED BATCH: 19 of 252\n",
      "FINSIHED BATCH: 39 of 252\n",
      "FINSIHED BATCH: 59 of 252\n",
      "FINSIHED BATCH: 79 of 252\n",
      "FINSIHED BATCH: 99 of 252\n",
      "FINSIHED BATCH: 119 of 252\n",
      "FINSIHED BATCH: 139 of 252\n",
      "FINSIHED BATCH: 159 of 252\n",
      "FINSIHED BATCH: 179 of 252\n",
      "FINSIHED BATCH: 199 of 252\n",
      "FINSIHED BATCH: 219 of 252\n",
      "FINSIHED BATCH: 239 of 252\n",
      "Validation loss per 100 evaluation steps: 0.49014678597450256\n",
      "DEV ACC: 0.8425191064079953\n",
      "DEV Precision: 0.7087238599143358\n",
      "DEV Recall: 0.7616276140085663\n",
      "DEV F1Score: 0.7122376790517728\n",
      "BEST ACCURACY -->  DEV: 0.84307\n",
      "BEST PRECISION -->  DEV: 0.70872\n",
      "BEST RECALL -->  DEV: 0.84938\n",
      "BEST F1SCORE -->  DEV: 0.73712\n",
      "TIME PER EPOCH: 6.861943165461223\n",
      "\n",
      "Training epoch: 5\n",
      "FINSIHED BATCH: 19 of 252\n",
      "FINSIHED BATCH: 39 of 252\n",
      "FINSIHED BATCH: 59 of 252\n",
      "FINSIHED BATCH: 79 of 252\n",
      "FINSIHED BATCH: 99 of 252\n",
      "FINSIHED BATCH: 119 of 252\n",
      "FINSIHED BATCH: 139 of 252\n",
      "FINSIHED BATCH: 159 of 252\n",
      "FINSIHED BATCH: 179 of 252\n",
      "FINSIHED BATCH: 199 of 252\n",
      "FINSIHED BATCH: 219 of 252\n",
      "FINSIHED BATCH: 239 of 252\n",
      "Validation loss per 100 evaluation steps: 0.4877963066101074\n",
      "DEV ACC: 0.8222369194591417\n",
      "DEV Precision: 0.6681058800106421\n",
      "DEV Recall: 0.8558167141500472\n",
      "DEV F1Score: 0.7331291681601275\n",
      "BEST ACCURACY -->  DEV: 0.84307\n",
      "BEST PRECISION -->  DEV: 0.70872\n",
      "BEST RECALL -->  DEV: 0.85582\n",
      "BEST F1SCORE -->  DEV: 0.73712\n",
      "TIME PER EPOCH: 6.86689309279124\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1\n",
      "FINSIHED BATCH: 19 of 252\n",
      "FINSIHED BATCH: 39 of 252\n",
      "FINSIHED BATCH: 59 of 252\n",
      "FINSIHED BATCH: 79 of 252\n",
      "FINSIHED BATCH: 99 of 252\n",
      "FINSIHED BATCH: 119 of 252\n",
      "FINSIHED BATCH: 139 of 252\n",
      "FINSIHED BATCH: 159 of 252\n",
      "FINSIHED BATCH: 179 of 252\n",
      "FINSIHED BATCH: 199 of 252\n",
      "FINSIHED BATCH: 219 of 252\n",
      "FINSIHED BATCH: 239 of 252\n",
      "Validation loss per 100 evaluation steps: 0.3186711370944977\n",
      "DEV ACC: 0.8184891240446797\n",
      "DEV Precision: 0.6243821874774255\n",
      "DEV Recall: 0.8847860693098786\n",
      "DEV F1Score: 0.7141067144727187\n",
      "BEST ACCURACY -->  DEV: 0.81849\n",
      "BEST PRECISION -->  DEV: 0.62438\n",
      "BEST RECALL -->  DEV: 0.88479\n",
      "BEST F1SCORE -->  DEV: 0.71411\n",
      "TIME PER EPOCH: 6.879524083932241\n",
      "\n",
      "Training epoch: 2\n",
      "FINSIHED BATCH: 19 of 252\n",
      "FINSIHED BATCH: 39 of 252\n",
      "FINSIHED BATCH: 59 of 252\n",
      "FINSIHED BATCH: 79 of 252\n",
      "FINSIHED BATCH: 99 of 252\n",
      "FINSIHED BATCH: 119 of 252\n",
      "FINSIHED BATCH: 139 of 252\n",
      "FINSIHED BATCH: 159 of 252\n",
      "FINSIHED BATCH: 179 of 252\n",
      "FINSIHED BATCH: 199 of 252\n",
      "FINSIHED BATCH: 219 of 252\n",
      "FINSIHED BATCH: 239 of 252\n",
      "Validation loss per 100 evaluation steps: 0.4527894854545593\n",
      "DEV ACC: 0.8367504409171076\n",
      "DEV Precision: 0.6649201768249386\n",
      "DEV Recall: 0.8899092970521542\n",
      "DEV F1Score: 0.7448576986650108\n",
      "BEST ACCURACY -->  DEV: 0.83675\n",
      "BEST PRECISION -->  DEV: 0.66492\n",
      "BEST RECALL -->  DEV: 0.88991\n",
      "BEST F1SCORE -->  DEV: 0.74486\n",
      "TIME PER EPOCH: 6.885819772879283\n",
      "\n",
      "Training epoch: 3\n",
      "FINSIHED BATCH: 19 of 252\n",
      "FINSIHED BATCH: 39 of 252\n",
      "FINSIHED BATCH: 59 of 252\n",
      "FINSIHED BATCH: 79 of 252\n",
      "FINSIHED BATCH: 99 of 252\n",
      "FINSIHED BATCH: 119 of 252\n",
      "FINSIHED BATCH: 139 of 252\n",
      "FINSIHED BATCH: 159 of 252\n",
      "FINSIHED BATCH: 179 of 252\n",
      "FINSIHED BATCH: 199 of 252\n",
      "FINSIHED BATCH: 219 of 252\n",
      "FINSIHED BATCH: 239 of 252\n",
      "Validation loss per 100 evaluation steps: 0.7026028037071228\n",
      "DEV ACC: 0.8491144914756026\n",
      "DEV Precision: 0.6907804717328526\n",
      "DEV Recall: 0.7078042328042327\n",
      "DEV F1Score: 0.6859711976503954\n",
      "BEST ACCURACY -->  DEV: 0.84911\n",
      "BEST PRECISION -->  DEV: 0.69078\n",
      "BEST RECALL -->  DEV: 0.88991\n",
      "BEST F1SCORE -->  DEV: 0.74486\n",
      "TIME PER EPOCH: 6.864515884717306\n",
      "\n",
      "Training epoch: 4\n",
      "FINSIHED BATCH: 19 of 252\n",
      "FINSIHED BATCH: 39 of 252\n",
      "FINSIHED BATCH: 59 of 252\n",
      "FINSIHED BATCH: 79 of 252\n",
      "FINSIHED BATCH: 99 of 252\n",
      "FINSIHED BATCH: 119 of 252\n",
      "FINSIHED BATCH: 139 of 252\n",
      "FINSIHED BATCH: 159 of 252\n",
      "FINSIHED BATCH: 179 of 252\n",
      "FINSIHED BATCH: 199 of 252\n",
      "FINSIHED BATCH: 219 of 252\n",
      "FINSIHED BATCH: 239 of 252\n",
      "Validation loss per 100 evaluation steps: 0.4564868211746216\n",
      "DEV ACC: 0.8493349500293945\n",
      "DEV Precision: 0.7215797430083146\n",
      "DEV Recall: 0.6729488765203053\n",
      "DEV F1Score: 0.6692764492484379\n",
      "BEST ACCURACY -->  DEV: 0.84933\n",
      "BEST PRECISION -->  DEV: 0.72158\n",
      "BEST RECALL -->  DEV: 0.88991\n",
      "BEST F1SCORE -->  DEV: 0.74486\n",
      "TIME PER EPOCH: 6.859784924983979\n",
      "\n",
      "Training epoch: 5\n",
      "FINSIHED BATCH: 19 of 252\n",
      "FINSIHED BATCH: 39 of 252\n",
      "FINSIHED BATCH: 59 of 252\n",
      "FINSIHED BATCH: 79 of 252\n",
      "FINSIHED BATCH: 99 of 252\n",
      "FINSIHED BATCH: 119 of 252\n",
      "FINSIHED BATCH: 139 of 252\n",
      "FINSIHED BATCH: 159 of 252\n",
      "FINSIHED BATCH: 179 of 252\n",
      "FINSIHED BATCH: 199 of 252\n",
      "FINSIHED BATCH: 219 of 252\n",
      "FINSIHED BATCH: 239 of 252\n",
      "Validation loss per 100 evaluation steps: 0.5167152881622314\n",
      "DEV ACC: 0.8493349500293945\n",
      "DEV Precision: 0.658332760713713\n",
      "DEV Recall: 0.8786542822257106\n",
      "DEV F1Score: 0.7375744939218839\n",
      "BEST ACCURACY -->  DEV: 0.84933\n",
      "BEST PRECISION -->  DEV: 0.72158\n",
      "BEST RECALL -->  DEV: 0.88991\n",
      "BEST F1SCORE -->  DEV: 0.74486\n",
      "TIME PER EPOCH: 6.860543962319692\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    n_epochs = 5\n",
    "    models = ['bert-base-multilingual-cased']\n",
    "    \n",
    "    #model saving parameters\n",
    "    model_save_flag = True\n",
    "    model_load_flag = False\n",
    "    \n",
    "    overall_list_dev_acc = [] \n",
    "    overall_list_test_acc = []    \n",
    "    overall_list_dev_precision = []  \n",
    "    overall_list_test_precision  = []  \n",
    "    overall_list_dev_recall = []  \n",
    "    overall_list_test_recall = []  \n",
    "    overall_list_dev_f1score = []  \n",
    "    overall_list_test_f1score = [] \n",
    "    \n",
    "    for i in range(5):\n",
    "        \n",
    "        for model_name in models:\n",
    "\n",
    "            model_save_location = 'saved_models/' + model_name + str(i)\n",
    "            model_load_location = None\n",
    "\n",
    "            best_dev_acc, best_test_acc, best_epoch, best_dev_precision, best_test_precision, best_dev_recall, best_test_recall, best_dev_f1score, best_test_f1score, list_dev_acc, list_test_acc, list_dev_precision, list_test_precision, list_dev_recall, list_test_recall, list_dev_f1score, list_test_f1score = main(n_epochs, model_name, model_save_flag, model_save_location, model_load_flag, model_load_location)\n",
    "            \n",
    "            overall_list_dev_acc.append(list_dev_acc) \n",
    "            overall_list_test_acc.append(list_test_acc) \n",
    "            overall_list_dev_precision.append(list_dev_precision)  \n",
    "            overall_list_test_precision.append(list_test_precision) \n",
    "            overall_list_dev_recall.append(list_dev_recall)  \n",
    "            overall_list_test_recall.append(list_test_recall)  \n",
    "            overall_list_dev_f1score.append(list_dev_f1score)  \n",
    "            overall_list_test_f1score.append(list_test_f1score) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "196e9dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8493349500293945\n"
     ]
    }
   ],
   "source": [
    "print(best_dev_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ced1188d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(best_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3a9b20d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7215797430083146\n"
     ]
    }
   ],
   "source": [
    "print(best_dev_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd177b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8899092970521542\n"
     ]
    }
   ],
   "source": [
    "print(best_dev_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa26b365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7448576986650108\n"
     ]
    }
   ],
   "source": [
    "print(best_dev_f1score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c373e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.8236331569664902, 0.8194811875367431, 0.8398184891240447, 0.8401859200470312, 0.8401308054085832], [0.8147045855379188, 0.8278218694885362, 0.8458627278071723, 0.8312022339800118, 0.841086125808348], [0.8347295708406819, 0.8225492357436802, 0.8419863315696648, 0.8422986478542034, 0.8356665196942975], [0.8180849500293945, 0.8365299823633157, 0.8430702527924749, 0.8425191064079953, 0.8222369194591417], [0.8184891240446797, 0.8367504409171076, 0.8491144914756026, 0.8493349500293945, 0.8493349500293945]]\n"
     ]
    }
   ],
   "source": [
    "print(overall_list_dev_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac29c8fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.6691403834260978, 0.7099521289997481, 0.7103237591332828, 0.6895920657825421, 0.6898686410591176], [0.6191982180077417, 0.7446082136558327, 0.6786911060720585, 0.663394718156623, 0.6492379313807886], [0.7025991204562633, 0.6329598531979487, 0.7139438603724317, 0.6624040862136099, 0.6605321926750498], [0.6484018186399139, 0.6690361666552144, 0.6888001328477519, 0.7087238599143358, 0.6681058800106421], [0.6243821874774255, 0.6649201768249386, 0.6907804717328526, 0.7215797430083146, 0.658332760713713]]\n"
     ]
    }
   ],
   "source": [
    "print(overall_list_dev_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b830b7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.8005165028974551, 0.5489103048626857, 0.7355568153187202, 0.7699105568153187, 0.7784706475182666], [0.8946694839551981, 0.6223293020912071, 0.8079805996472663, 0.8277955289860051, 0.9022234819853866], [0.8053539934492315, 0.9126417233560091, 0.6653088534040915, 0.8693625598387502, 0.7997480473670948], [0.8416099773242628, 0.84937641723356, 0.8306248425296043, 0.7616276140085663, 0.8558167141500472], [0.8847860693098786, 0.8899092970521542, 0.7078042328042327, 0.6729488765203053, 0.8786542822257106]]\n"
     ]
    }
   ],
   "source": [
    "print(overall_list_dev_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9109a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.7124427001951694, 0.601333322761894, 0.7057421906861681, 0.7127568740387549, 0.708495612697293], [0.7148121086791288, 0.6592020413448986, 0.7232984944137825, 0.7188054398138431, 0.7438834187691623], [0.7340268404694171, 0.7319314151379928, 0.6690647597370926, 0.7402936333682314, 0.7052597690884583], [0.7200731236482377, 0.7340460216584346, 0.7371211682099192, 0.7122376790517728, 0.7331291681601275], [0.7141067144727187, 0.7448576986650108, 0.6859711976503954, 0.6692764492484379, 0.7375744939218839]]\n"
     ]
    }
   ],
   "source": [
    "print(overall_list_dev_f1score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "319d8292",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best model is model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61540c34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
